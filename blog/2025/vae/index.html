<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Stable Diffusion Series 4/5 - Variational Auto-Encoders | Yasser KHALAFAOUI</title> <meta name="author" content="Yasser KHALAFAOUI"> <meta name="description" content="Why do we need an auto-encoder for Stable Diffusion ?"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-32x32.png?6c7b1b665db8390fc2f1e79e8ba0f8d0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mkdyasserh.github.io/blog/2025/vae/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Stable Diffusion Series 4/5 - Variational Auto-Encoders",
      "description": "Why do we need an auto-encoder for Stable Diffusion ?",
      "published": "January 12, 2025",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yasser </span>KHALAFAOUI</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link text-uppercase" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link text-uppercase" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link text-uppercase" href="/contact/">contact</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Stable Diffusion Series 4/5 - Variational Auto-Encoders</h1> <p>Why do we need an auto-encoder for Stable Diffusion ?</p> </d-title><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#the-need-for-efficient-data-representations">The Need for Efficient Data Representations</a></div> <div><a href="#what-are-autoencoders">What Are Autoencoders?</a></div> <div><a href="#convolutional-autoencoders">Convolutional Autoencoders</a></div> <div><a href="#variational-autoencoders">Variational Autoencoders</a></div> <div><a href="#the-reparameterization-trick">The Reparameterization Trick</a></div> <div><a href="#why-not-train-%CF%B5">Why Not Train ϵ?</a></div> <div><a href="#vae-equation">VAE Equation</a></div> <ul> <li><a href="#reconstruction-loss">Reconstruction Loss</a></li> <li><a href="#regularization-loss">Regularization Loss</a></li> </ul> <div><a href="#residual-blocks">Residual Blocks</a></div> <div><a href="#implementation">Implementation</a></div> </nav> </d-contents> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vae_ae.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="the-need-for-efficient-data-representations">The Need for Efficient Data Representations</h1> <p>Imagine I give you two number sequences and ask which one is easier to remember:</p> <ol> <li> <strong>Sequence A -</strong> 3, 7, 12, 19, 24</li> <li> <strong>Sequence B -</strong> 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29</li> </ol> <p>Although Sequence A has fewer numbers, each one is unique and independent. You’d have to remember each number individually. On the other hand, while Sequence B is longer, it follows a simple pattern: it’s the sequence of odd numbers from 1 to 29. This pattern makes it much easier to remember because instead of memorizing all 15 numbers, you only need to store the starting number (1), the ending number (29), and the rule (“odd numbers”).</p> <p>This analogy highlights the concept of efficient data representations. Instead of storing or processing all the details of the data, we seek a compact and structured way to capture the essential features, reducing redundancy and preserving the key information. This is especially useful when dealing with high-dimensional data like images, audio, or text. Efficient representations enable models to focus on meaningful patterns and relationships within the data, leading to better performance in tasks such as classification, clustering, and generation.</p> <h1 id="what-are-autoencoders">What Are Autoencoders?</h1> <p>Autoencoders are a type of neural network architecture designed to learn efficient data representations in an unsupervised manner. Unlike supervised learning, where models learn to map inputs to specific outputs, autoencoders learn to encode the input data into a compressed form and then decode it back to reconstruct the original input. This process helps them identify and retain the most critical features while discarding irrelevant information.</p> <p>The architecture of a basic autoencoder consists of two main parts:</p> <ul> <li> <strong>Encoder -</strong> The encoder compresses the input data into a lower-dimensional space known as the latent representation. This compression forces the network to learn the essential characteristics of the input data and discard the less significant details.</li> <li> <strong>Decoder -</strong> The decoder takes the latent representation and reconstructs it back to match the original input as closely as possible. This reconstruction step ensures that the encoder has effectively captured the underlying structure of the input data.</li> </ul> <p>Visually, autoencoders resemble a typical neural network with an input layer, hidden layers, and an output layer. However, unlike other neural networks, the number of neurons in the output layer of an autoencoder must be equal to the number of inputs to ensure it can reconstruct the original data.</p> <p>Since the latent representation has a lower dimensionality than the input data, the network is unable to simply copy the input features into the latent space. Instead, it’s forced to learn the most salient features and representations that summarize the input. This makes autoencoders highly efficient at dimensionality reduction and feature extraction.</p> <p>Moreover, autoencoders can be used as generative models. By training on a given dataset, they learn to generate new data samples that resemble the original training data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/traditional_ae.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Visual representation of an autoencoder. </div> <h1 id="convolutional-autoencoders">Convolutional Autoencoders</h1> <p>In our previous discussions on images, we highlighted how convolutional layers are more effective than dense layers for capturing spatial hierarchies and local features. When building autoencoders for images, it’s beneficial to use Convolutional Autoencoders (CAEs) instead of fully connected ones. This allows us to leverage the strengths of convolutional operations, such as detecting edges, textures, and shapes, while preserving spatial structure.</p> <p>But rest assured, the core architecture—comprising the encoder, latent representation, and decoder—remains the same with some minor modifications. The key difference is how these components are constructed:</p> <ul> <li> <strong>Encoder -</strong> The encoder in a convolutional autoencoder is similar to that of a standard CNN. It consists of a series of convolutional and pooling layers (or downsampling layers). As with a typical CNN, the purpose of the encoder is to progressively reduce the height and width of the image while increasing its depth, meaning it transforms the original image into a set of lower-dimensional feature maps. These feature maps capture the key patterns and spatial structures of the image in a compact form.</li> <li> <strong>Decoder -</strong> The decoder reverses the transformations applied by the encoder. It uses upsampling layers (like transposed convolutions or interpolation layers) to gradually increase the height and width of the feature maps while reducing their depth, reconstructing the input image. The objective is to restore the image’s original dimensions and structure as accurately as possible, based on the compressed latent representation.</li> </ul> <p>For now, we won’t delve into the implementation details until we cover the theoretical aspects more thoroughly. However, later in this chapter, we’ll show how convolutional autoencoders can be tailored for more complex tasks such as Stable Diffusion. In fact, Stable Diffusion uses convolutional autoencoders as part of its architecture. We’ll explore how and why these models are used in greater detail as we proceed.</p> <h1 id="variational-autoencoders">Variational Autoencoders</h1> <p>Variational Autoencoders (VAEs) are one of the most widely used and popular types of autoencoders, introduced in the paper <d-cite key="kingma2013auto"></d-cite>. What sets them apart from traditional autoencoders is their probabilistic nature and ability to generate new data points that resemble the training data.</p> <p>The VAE plays a crucial role in diffusion model architecture. The encoder produces the latent representation of an input image, in the case of image-to-image, or a noisy input in the case of text-to-image, while the decoder is the final step that reconstructs and outputs the image you requested with a text prompt.</p> <p>The key distinction between a VAE and a standard autoencoder is that instead of mapping the input to a fixed latent vector, VAEs aim to map it to a distribution. More precisely, the latent vector is replaced by two components: the mean \(\mu\) and the standard deviation \(\sigma\). From this Gaussian distribution, defined by \(\mu\) and \(\sigma\), we sample a latent vector, which is then passed to the decoder to reconstruct the input, just like in traditional autoencoders. This leads to two important characteristics of VAEs:</p> <ul> <li> <strong>Probabilistic Nature -</strong> The latent space of VAEs is probabilistic, meaning that each time we sample from the Gaussian distribution, we get slightly different latent representations. As a result, the output of the decoder is also probabilistic, introducing variability in the reconstructions.</li> <li> <strong>Generative Capabilities -</strong> Because of their stochastic nature, VAEs are generative models. They can generate new instances that look similar to the data they were trained on but are, in fact, unique. For example, if a VAE is trained on images of dogs, it could generate a new image of a dog that resembles the training data but includes subtle variations, like a dog with three ears. These new samples come from the same underlying distribution as the training data but introduce novel variations.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/variational.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As illustrated in the diagram above, after the encoder processes the input, the final layer of the encoder outputs two vectors: \(\mu\) and \(\sigma\) of the latent distribution. Instead of directly sampling from these vectors, an additional step is introduced — <strong>the reparameterization trick</strong>.</p> <h1 id="the-reparameterization-trick">The Reparameterization Trick</h1> <p>In the ideal scenario, we would sample directly from the latent distribution using the computed \(\mu\) and \(\sigma\). However, during training, we use backpropagation to compute the gradient of the loss with respect to every trainable parameter in the network. The stochastic nature of \(\mu\) and \(\sigma\) creates a problem here, as the gradients cannot propagate through the sampling step.</p> <p>The solution is the reparameterization trick, which introduces a clever workaround: Instead of sampling directly from the Gaussian distribution defined by \(\mu\) and \(\sigma\), we transform the process into something more manageable for backpropagation. Here’s how it works:</p> <ol> <li>We sample an auxiliary variable \(\epsilon\) from a standard normal distribution with \(\mu = 0\) and \(\sigma = 1\).</li> <li>The latent vector is then computed as \(z=μ+σ\ast \epsilon\). This operation allows the mean and standard deviation to remain deterministic, which makes them suitable for gradient-based learning. The stochastic aspect is now captured by \(\epsilon\).</li> </ol> <p>By using this trick, we move the randomness into a non-trainable node \(\epsilon\) while keeping \(\mu\) and \(\sigma\) as trainable parameters, allowing gradients to flow during backpropagation.</p> <h1 id="why-not-train-epsilon">Why Not Train \(\epsilon\)?</h1> <p>A common question might arise: “Why not train \(\epsilon\)?” The answer is simple — \(\epsilon\) is sampled fresh in every forward pass, acting as a fixed noise source. Its role is to introduce controlled randomness, and it doesn’t need to be learned since it always comes from the standard Gaussian distribution (mean 0, standard deviation 1).</p> <h1 id="vae-equation">VAE Equation</h1> <p>Now that we’ve covered how VAEs differ from traditional autoencoders, let’s dive deeper into their loss function. The VAE loss combines two important components that work together to ensure the model can generate new data while keeping the latent space well-organized.</p> <p>Below is the general formula for the VAE loss:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/vae_equation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>It consists of two parts:</p> <ul> <li> <strong>Reconstruction Loss -</strong> The first term helps the model recreate the input data as accurately as possible.</li> <li> <strong>Regularization Loss -</strong> The second term, also known as the <strong>Kullback-Leibler (KL) divergence</strong>, penalizes the model if the latent space deviates too much from the normal distribution.</li> </ul> <p>Let’s break these components down further.</p> <h2 id="reconstruction-loss">Reconstruction Loss</h2> <p>The primary goal of the VAE is to take an input (e.g., an image), encode it into a simpler latent representation, and then decode it to reconstruct the original input. The reconstruction loss measures how close the reconstructed output is to the original input.</p> <p>To make this clearer: Suppose you give the model a picture of a cat. The model compresses the image into a latent vector and then reconstructs it. The reconstruction loss checks how similar the new image is to the original. The more accurate the reconstruction, the smaller this loss becomes.</p> <p>Technically, this term is the <strong>likelihood</strong> of the original input \(x\) given the latent representation \(z\) from the encoder. We aim to maximize this likelihood so that the decoder, represented by \(p_\theta(x∣z)\), can generate data as close as possible to the original input, which was compressed by the encoder \(q_\phi(z∣x)\).</p> <h2 id="regularization-loss">Regularization Loss</h2> <p>The second part of the VAE loss function is the regularization term, which is where the probabilistic nature of VAEs comes into play. Recall that the encoder doesn’t output a single deterministic latent vector but instead generates a mean \(\mu\) and standard deviation \(\sigma\), allowing us to sample from a Gaussian distribution to create the latent representation.</p> <p>This stochastic property adds flexibility, allowing the VAE to generate smooth, continuous variations of new data. However, to ensure that the latent space is well-structured and meaningful, we need to regularize it. This is achieved through the KL divergence, which measures how much the learned latent distribution, produced by the encoder, deviates from a standard Gaussian distribution. The goal is to make the latent vectors follow this normal distribution so that similar inputs produce similar latent representations. If the latent space is poorly organized — i.e., if latent vectors are scattered randomly — generating new, coherent data points would be difficult. The KL divergence penalizes the model when the latent vectors stray too far from the Gaussian distribution, encouraging a well-organized and continuous latent space.</p> <p>In summary, the regularization term ensures that the latent space remains structured, preventing it from becoming chaotic, and helping the model generate meaningful and smooth variations from the training data.</p> <h1 id="residual-blocks">Residual Blocks</h1> <p>Residual blocks, or skip connections, were a game-changing innovation introduced in the ResNet paper <d-cite key="he2016deep"></d-cite>. They addressed a rather surprising issue in deep learning: adding more layers to a neural network doesn’t always lead to better performance. In fact, as networks grew deeper, their performance often degraded. This ran contrary to the fundamental idea of deep learning—that deeper networks should be able to capture more complex features and perform better.</p> <p>So, what was going wrong? As networks became deeper, they struggled to learn optimal weights. Information struggles to pass through all the layers, and the gradients during backpropagation became increasingly small, leading to what’s known as the <strong>vanishing gradient problem</strong>. This made it difficult for the network to update weights, especially in the earlier layers, causing deep models to underperform compared to shallower ones.</p> <p>Residual blocks provided a clever solution to this problem. Rather than forcing the network to directly learn the full mapping \(f(x)\), residual blocks allow the network to learn the residual \(h(x)=f(x)-x\), which simplifies to \(f(x)=h(x)+x\). The key is an identity connection — a shortcut — that bypasses one or more layers, letting information <em>skip</em> through the network without obstruction. This allows gradients to flow more freely during backpropagation, addressing the vanishing gradient issue.</p> <p>But the benefits of residual blocks don’t stop there. These identity connections also speed up training. Early in training, when weights are initialized near zero, approximating the identity function provides a helpful kickstart. In traditional networks, some layers may struggle to <em>“wake up”</em> because of small gradients or poor initialization. But with residual blocks, the model can start by learning the identity function \(f(x)\approx x\) , which allows it to make rapid initial progress before moving on to learn more complex mappings. This ensures that the model doesn’t get stuck early on, making learning faster and more efficient.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/res_block.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Now, you might wonder: why are residual blocks important in the context of stable diffusion? In architectures like Stable Diffusion, residual blocks play a critical role in maintaining a smooth flow of information through the deep layers of the network. Generating high-quality images depends on this effective information flow. By incorporating residual connections within the autoencoders, the model ensures that transformations in the latent space remain consistent and stable, even as it manipulates intricate details in images.</p> <h1 id="implementation">Implementation</h1> <p>In this section, we will implement the building blocks of a VAE for Stable Diffusion. Specifically, we’ll focus on the encoder, decoder, and the all-important residual block. By the end, you’ll not only understand how these components work but also how they come together to form a functional VAE.</p> <p>Before diving into the encoder and decoder implementation, let’s first look at the residual block which is implemented as follows:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ResBlock</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># Layers to process the input features
</span>        <span class="n">self</span><span class="p">.</span><span class="n">in_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">GroupNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span>
            <span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">swish</span><span class="p">,</span>
            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Layers to process the time embedding
</span>        <span class="n">self</span><span class="p">.</span><span class="n">emb_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">swish</span><span class="p">,</span>
            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Layers to further refine the merged features
</span>        <span class="n">self</span><span class="p">.</span><span class="n">out_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">GroupNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span>
            <span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">swish</span><span class="p">,</span>
            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Skip connection for residual learning
</span>        <span class="n">self</span><span class="p">.</span><span class="n">skip_connection</span> <span class="o">=</span> <span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="k">if</span> <span class="n">in_channels</span> <span class="o">==</span> <span class="n">out_channels</span> <span class="k">else</span> <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># Unpack the inputs: feature maps and time embedding
</span>        <span class="n">z</span><span class="p">,</span> <span class="n">time</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">residue</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># Save the input for the skip connection
</span>
        <span class="c1"># Apply the input layers
</span>        <span class="n">z</span> <span class="o">=</span> <span class="nf">apply_seq</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">in_layers</span><span class="p">)</span>

        <span class="c1"># Process the time embedding
</span>        <span class="n">time</span> <span class="o">=</span> <span class="nf">apply_seq</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">emb_layers</span><span class="p">)</span>
        
        <span class="c1"># Merge the feature maps with the time embedding
</span>        <span class="n">merged</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">time</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>

        <span class="c1"># Apply the output layers
</span>        <span class="n">merged</span> <span class="o">=</span> <span class="nf">apply_seq</span><span class="p">(</span><span class="n">merged</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_layers</span><span class="p">)</span>

        <span class="c1"># Add the skip connection and return the result
</span>        <span class="k">return</span> <span class="n">merged</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">skip_connection</span><span class="p">(</span><span class="n">residue</span><span class="p">)</span></code></pre></figure> <p>The Residual Block uses a skip connection, which helps retain information from earlier layers. It also integrates time embeddings which are essential for the Diffusion model.</p> <p>Now moving on to the encoder, which is the component responsible for transforming the input image into a latent representation. Below is the implementation in TensorFlow.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>

<span class="c1"># Define the encoder for the Variational Autoencoder
</span><span class="k">class</span> <span class="nc">VAE_Encoder</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">([</span>
            <span class="c1"># Initial convolution to extract 128 features
</span>            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># (batch_size, 128, height, width)
</span>
            <span class="c1"># Stack of ResNet blocks to refine features
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>  <span class="c1"># (batch_size, 128, height, width)
</span>
            <span class="c1"># Downsample using strided convolution
</span>            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>  <span class="c1"># (batch_size, 128, height/2, width/2)
</span>
            <span class="c1"># Further feature extraction and dimension increase
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>  <span class="c1"># (batch_size, 256, height/2, width/2)
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>

            <span class="c1"># Downsample again
</span>            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>  <span class="c1"># (batch_size, 256, height/4, width/4)
</span>
            <span class="c1"># Continue with higher-dimensional feature extraction
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>  <span class="c1"># (batch_size, 512, height/4, width/4)
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>

            <span class="c1"># Final downsampling to reduce spatial dimensions
</span>            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>  <span class="c1"># (batch_size, 512, height/8, width/8)
</span>
            <span class="c1"># Deep feature extraction using multiple ResNet blocks
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>

            <span class="c1"># Attention block for contextual feature aggregation
</span>            <span class="nc">AttentionBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">),</span>

            <span class="c1"># Additional refinement with ResNet block
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>

            <span class="c1"># Normalize and activate features
</span>            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">GroupNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span>
            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">swish</span><span class="sh">'</span><span class="p">),</span>

            <span class="c1"># Final convolution to reduce feature dimensions
</span>            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># (batch_size, 8, height/8, width/8)
</span>            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># (batch_size, 8, height/8, width/8)
</span>
            <span class="c1"># Scale latent representation
</span>            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.18215</span><span class="p">),</span>
        <span class="p">])</span></code></pre></figure> <p>Inversely, the decoder takes the latent representation and reconstructs the image by progressively upsampling and refining the features.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Define the decoder for the Variational Autoencoder
</span><span class="k">class</span> <span class="nc">VAE_Decoder</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">([</span>
            <span class="c1"># Rescale the latent input
</span>            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">0.18215</span> <span class="o">*</span> <span class="n">x</span><span class="p">),</span>

            <span class="c1"># Initial convolution to expand features
</span>            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>

            <span class="c1"># Stack of ResNet and Attention blocks to refine features
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="nc">AttentionBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>

            <span class="c1"># Upsample spatial dimensions
</span>            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">UpSampling2D</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>

            <span class="c1"># Further refinement with ResNet blocks
</span>            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>

            <span class="c1"># Upsample and refine again
</span>            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">UpSampling2D</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>

            <span class="c1"># Final upsampling to original image dimensions
</span>            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">UpSampling2D</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="nc">ResnetBlock</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>

            <span class="c1"># Final normalization and activation
</span>            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">GroupNormalization</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
            <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">swish</span><span class="sh">'</span><span class="p">),</span>

            <span class="c1"># Final convolution to map back to RGB channels
</span>            <span class="nc">PaddedConv2D</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># (batch_size, 3, height, width)
</span>        <span class="p">])</span></code></pre></figure> <p>Here are some more articles you might like to read next:</p> <ul> <li><a href="../../2025/stable-diffusion">Stable Diffusion Series 5/5 - Exploring Diffusion, Classifier-Free Guidance, UNET, and CLIP</a></li> <li><a href="../../2025/intro">Stable Diffusion Series 1/5 - Introduction and Prerequisites</a></li> <li><a href="../../2025/convolution">Stable Diffusion Series 2/5 - Convolution Layers Explained</a></li> <li><a href="../../2025/attention">Stable Diffusion Series 3/5 - Attention Mechanisms Explained</a></li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"MKDYasserH/mkdyasserh.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yasser KHALAFAOUI. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-QJB042V4LQ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-QJB042V4LQ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>